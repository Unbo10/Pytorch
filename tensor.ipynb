{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "Tensors are mathematical objects, don't forget that. In Pytorch, you will probably see them as multi-dimensional arrays of real numbers, and in general, that's what they are most used as. However, they could contain any mathematical object that is part of a vector space with its corresponding operations and field.\n",
    "\n",
    "In Pytorch, tensors are objects (just like everything else in Python), and they are initialized with a multi-dimensional array of numbers (except for the scalar tensors). There are four types: scalars, vectors, matrices and tensors (3D and above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALAR\n",
      "tensor(7)\n",
      "0\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"SCALAR\")\n",
    "scalar: torch.Tensor = torch.tensor(7)\n",
    "print(scalar)\n",
    "print(scalar.ndim)\n",
    "print(scalar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ``__str__`` magic method prints the definition of the tensor without the torch module.\n",
    "- The ``ndim`` attribute returns the number of dimensions of the tensor or its rank.\n",
    "  > You could see it as the amount of sub-indices you need to point to one component. For ex., the components of a vector will only need one to point to any axis of the vector's coordinate system, whereas matrices components will require two: One pointing to the column (vector) and one pointing to some axis of the coordinate system. In consequence, scalars won't need subindices because they need to point to the same system that is a numeric system, and so they have dimension 0.\n",
    "  * N. dimensions = Number of square brackets ([]).\n",
    "- The ``shape`` attribute returns the size of the tensor in each dimension (it's an alias for the `size` attribute).\n",
    "  > It returns the number of components in each dimension. You could see it as the amount of values each subindex could take.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VECTORS\n",
      "tensor([1, 0, 1])\n",
      "1\n",
      "torch.Size([3])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(\"VECTORS\")\n",
    "vector: torch.Tensor = torch.tensor([1, 0, 1])\n",
    "print(vector)\n",
    "print(vector.ndim)\n",
    "print(vector.shape)\n",
    "print(vector[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can access the components of a tensor by using the square brackets notation. For this purpose, just think of the tensor as an array.\n",
    "  > Think of the indices inside the brackets as the one to access a component of a tensor. You may access another tensor or a scalar. Be mindful, since you lock or select a component of a dimension per bracket, each bracket will decrease the rank or dimension of the tensor by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> WARNING: The tensors are not printed as matrices, but as arrays. That means that you should read the rows as columns and the columns as rows. This is because the tensors are not matrices, but multi-dimensional arrays in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRICES\n",
      "tensor([[  1,   2,  -3,   4],\n",
      "        [  5,  -6,  -7,   8],\n",
      "        [-10, -20,  30,  40],\n",
      "        [  0,   0,   0,   0]])\n",
      "2\n",
      "torch.Size([4, 4])\n",
      "tensor([0, 0, 0, 0])\n",
      "tensor(1)\n",
      "tensor([[2., 2., 3.],\n",
      "        [2., 3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"MATRICES\")\n",
    "matrix: torch.Tensor = torch.tensor([[1, 2, -3, 4], [5, -6, -7, 8], [-10, -20, 30, 40], [0, 0, 0, 0]])\n",
    "print(matrix)\n",
    "print(matrix.dim()) #* Equivalent to .ndim\n",
    "print(matrix.size()) #* Equivalent to .shape\n",
    "print(matrix[3]) #* Vector in the fourth component\n",
    "print(matrix[0][0]) #* First scalar of the vector in the first component.\n",
    "matrix2: torch.Tensor = torch.tensor(data=[[2, 2], [2, 3]], dtype=torch.float)\n",
    "# matrix2 = matrix2.float()\n",
    "print(torch.det(matrix2)) #* Determinant of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.det()` calculates the determinant of a matrix. It should be noted that the tensor must have floating-point data type, which can be achieved by setting the parameter `dtype` to any floating-point `dtype` (e.g., `torch.float32`), by using the `.float()` or `.double()` methods of `Tensor` (equivalent to `self.to(torch.float32)` and `self.to(torch.float64)`, respectively), or by adding a decimal point to at least one entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, despite the function `torch.tensor()` returning an instance of the `torch.Tensor` class, it is different from calling the constructor of the `Tensor` class:\n",
    "  - `tensor()` accepts the `device` argument which allows you to specify where the tensor will be stored (CPU or GPU), whereas the constructor does not.\n",
    "  - By default, any tensor created with `tensor()` will have the `requires_grad` attribute set to `False`, i.e., a **leaf tensor**. In Pytorch, this means the tensor doesn't use the autograd engine to compute gradients (SHOULD DIVE DEEPER INTO THIS). In contrast, the constructor will set this attribute to `True`.\n",
    "  - The `dtype` argument is also exclusively accepted by `tensor()`, which allows you to specify the data type of the tensor. However, if not specified, it will infer the data type from the input data.\n",
    "- The key takeaway from the docs is that the `Tensor` class is a base class and initializing them with the constructor is \"discouraged\". Multiple ways of creating a tensor are provided [here](https://pytorch.org/docs/stable/tensors.html#tensor-class-reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORS (3D+)\n",
      "tensor([[[0.4816, 0.6169, 0.4486],\n",
      "         [0.5777, 0.0164, 0.9253],\n",
      "         [0.9609, 0.2100, 0.9207]],\n",
      "\n",
      "        [[0.1376, 0.7634, 0.9645],\n",
      "         [0.7217, 0.6080, 0.6722],\n",
      "         [0.0276, 0.5667, 0.4058]]])\n",
      "tensor([[[[0.2580, 0.8096, 0.9096],\n",
      "          [0.7735, 0.1770, 0.2787]],\n",
      "\n",
      "         [[0.3462, 0.2957, 0.7506],\n",
      "          [0.6468, 0.5747, 0.4183]]],\n",
      "\n",
      "\n",
      "        [[[0.7337, 0.0637, 0.9802],\n",
      "          [0.8396, 0.7473, 0.1529]],\n",
      "\n",
      "         [[0.6176, 0.9156, 0.7299],\n",
      "          [0.4805, 0.5894, 0.2401]]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"TENSORS (3D+)\")\n",
    "tensor_r3: torch.Tensor = torch.rand([2, 3, 3])\n",
    "print(tensor_r3)\n",
    "tensor_r4: torch.Tensor = torch.rand([2, 2, 2, 3])\n",
    "print(tensor_r4) #* Rank 4 tensor containing two rank 3 tensors that contain two matrices with two shape 3 (3 axis) vectors each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, creating higher-dimensional or higher-ranked tensors is just a matter of adding more square brackets, and to intepret a rank-nth tensor as a collection of rank-(n-1)th tensors and such as a collection of rank-(n-2)th tensors and so on, until you reach scalars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.rand(Sequence[int])` is a way to create a tensor with random values from a uniform distribution in the range [0, 1) and shape `Sequence[int]`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
