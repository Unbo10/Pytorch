{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "Tensors are mathematical objects, don't forget that. In Pytorch, you will probably see them as multi-dimensional arrays of real numbers, and in general, that's what they are most used as. However, they could contain any mathematical object that is part of a vector space with its corresponding operations and field.\n",
    "\n",
    "In Pytorch, tensors are objects (just like everything else in Python), and they are initialized with a multi-dimensional array of numbers (except for the scalar tensors). There are four types: scalars, vectors, matrices and tensors (3D and above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALAR\n",
      "tensor(7)\n",
      "0\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"SCALAR\")\n",
    "scalar: torch.Tensor = torch.tensor(7)\n",
    "print(scalar)\n",
    "print(scalar.ndim)\n",
    "print(scalar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ``__str__`` magic method prints the definition of the tensor without the torch module, the dtype if not float32, the device if not CPU, and the last operation on the tensor if requires grad is set to True.\n",
    "- The ``ndim`` attribute returns the number of dimensions of the tensor or its rank.\n",
    "  > You could see it as the amount of sub-indices you need to point to one component. For ex., the components of a vector will only need one to point to any axis of the vector's coordinate system, whereas matrices components will require two: One pointing to the column (vector) and one pointing to some axis of the coordinate system. In consequence, scalars won't need subindices because they need to point to the same system that is a numeric system, and so they have dimension 0.\n",
    "  * N. dimensions = Number of square brackets ([]).\n",
    "- The ``shape`` attribute returns the size of the tensor in each dimension (it's an alias for the `size` attribute).\n",
    "  > It returns the number of components in each dimension. You could see it as the amount of values each subindex could take.\n",
    "  - You can infer the 'size' of each dimension by counting the amount of commas there is within a certain pair of square brackets (excluding the ones inside their content).\n",
    "  - Note that from left to right, the size refers to the most outer to the most inner dimension. For example, if a tensor has size [2, 3, 4], when ignoring the first pair of brackets, there will be two elements; ignoring the second pair, there will be three elements; and ignoring the third pair, there will be four elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VECTORS\n",
      "tensor([1, 0, 1])\n",
      "1\n",
      "torch.Size([3])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(\"VECTORS\")\n",
    "vector: torch.Tensor = torch.tensor([1, 0, 1])\n",
    "print(vector)\n",
    "print(vector.ndim)\n",
    "print(vector.shape)\n",
    "print(vector[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can access the components of a tensor by using the square brackets notation. For this purpose, just think of the tensor as an array.\n",
    "  > Think of the indices inside the brackets as the one to access a component of a tensor. You may access another tensor or a scalar. Be mindful, since you lock or select a component of a dimension per bracket, each bracket will decrease the rank or dimension of the tensor by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> WARNING: The tensors are not printed as matrices, but as arrays. That means that you should read the rows as columns and the columns as rows. This is because the tensors are not matrices, but multi-dimensional arrays in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices (Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRICES\n",
      "tensor([[  1,   2,  -3,   4],\n",
      "        [  5,  -6,  -7,   8],\n",
      "        [-10, -20,  30,  40],\n",
      "        [  0,   0,   0,   0]])\n",
      "2\n",
      "torch.Size([4, 4])\n",
      "tensor([0, 0, 0, 0])\n",
      "tensor(1)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "print(\"MATRICES\")\n",
    "matrix: torch.Tensor = torch.tensor([[1, 2, -3, 4], [5, -6, -7, 8], [-10, -20, 30, 40], [0, 0, 0, 0]])\n",
    "print(matrix)\n",
    "print(matrix.ndim) #* Equivalent to .ndim\n",
    "print(matrix.size()) #* Equivalent to .shape\n",
    "print(matrix[3]) #* Vector in the fourth component\n",
    "print(matrix[0][0]) #* First scalar of the vector in the first component.\n",
    "matrix2: torch.Tensor = torch.tensor(data=[[2, 2], [2, 3]], dtype=torch.float)\n",
    "# matrix2 = matrix2.float()\n",
    "print(torch.det(matrix2)) #* Determinant of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.det()` calculates the determinant of a matrix. It should be noted that the tensor must have floating-point data type, which can be achieved by setting the parameter `dtype` to any floating-point `dtype` argument (e.g., `torch.float32`), by using the `.float()` or `.double()` methods of `Tensor` (equivalent to `self.to(torch.float32)` and `self.to(torch.float64)`, respectively), or by adding a decimal point to at least one entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, despite the function `torch.tensor()` returning an instance of the `torch.Tensor` class, it is different from calling the constructor of the `Tensor` class:\n",
    "  - `tensor()` accepts the `device` argument which allows you to specify where the tensor will be stored (CPU or GPU), whereas the constructor does not.\n",
    "  - By default, any tensor created with `tensor()` will have the `requires_grad` attribute set to `False`, i.e., a **leaf tensor**. In Pytorch, this means the tensor doesn't use the autograd engine to compute gradients (SHOULD DIVE DEEPER INTO THIS). In contrast, the constructor will set this attribute to `True`.\n",
    "  - The `dtype` argument is also exclusively accepted by `tensor()`, which allows you to specify the data type of the tensor. However, if not specified, it will infer the data type from the input data.\n",
    "- The key takeaway from the docs is that the `Tensor` class is a base class and initializing them with the constructor is \"discouraged\". Multiple ways of creating a tensor are provided [here](https://pytorch.org/docs/stable/tensors.html#tensor-class-reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORS (3D+)\n",
      "tensor([[[0.0359, 0.9667, 0.0933],\n",
      "         [0.9923, 0.0734, 0.3498],\n",
      "         [0.4092, 0.1741, 0.9880]],\n",
      "\n",
      "        [[0.6884, 0.9304, 0.0443],\n",
      "         [0.8843, 0.9370, 0.6180],\n",
      "         [0.1633, 0.4733, 0.7147]]])\n",
      "3\n",
      "tensor([[[[0.7087, 0.3501, 0.6360],\n",
      "          [0.3732, 0.8799, 0.5636]],\n",
      "\n",
      "         [[0.7067, 0.4211, 0.9650],\n",
      "          [0.4375, 0.7758, 0.7086]]],\n",
      "\n",
      "\n",
      "        [[[0.0436, 0.5770, 0.3921],\n",
      "          [0.1392, 0.2634, 0.8835]],\n",
      "\n",
      "         [[0.8987, 0.7395, 0.0448],\n",
      "          [0.2507, 0.6783, 0.6747]]]])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(\"TENSORS (3D+)\")\n",
    "tensor_r3: torch.Tensor = torch.rand([2, 3, 3])\n",
    "print(tensor_r3)\n",
    "print(tensor_r3.ndim)\n",
    "tensor_r4: torch.Tensor = torch.rand(2, 2, 2, 3) #* There is no need to put the sizes in a list, but I assume you would have to if you needed to pass more arguments to rand()\n",
    "print(tensor_r4) #* Rank 4 tensor containing two rank 3 tensors that contain two matrices with two shape 3 (3 axis) vectors each\n",
    "print(tensor_r4.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, creating higher-dimensional or higher-ranked tensors is just a matter of adding more square brackets, and to intepret a rank-nth tensor as a collection of rank-(n-1)th tensors and such as a collection of rank-(n-2)th tensors and so on, until you reach scalars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor (test)\n",
      "tensor([[0.8960, 0.2513, 0.4274, 0.6678],\n",
      "        [0.8142, 0.0179, 0.3338, 0.1075],\n",
      "        [0.8935, 0.4005, 0.3862, 0.3361]])\n",
      "Using the zero_() function on the random tensor\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "Converting the previous tensor of zeros into one of ones tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "New tensor of zeroes using zeros()\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "New tensor of ones using ones()\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([1, 3, 5, 7])\n",
      "torch.Size([4]) 1\n",
      "Supposedly an uninitialized tensor with the same shape as test\n",
      "tensor([[4.9487e-14, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [       nan, 0.0000e+00, 1.1578e+27, 7.1463e+22],\n",
      "        [4.6241e+30, 1.0552e+24, 5.5757e-02, 1.8728e+31]])\n",
      "A tensor with the same shape as test and with all of its entries = 8\n",
      "tensor([[8, 8, 8, 8],\n",
      "        [8, 8, 8, 8],\n",
      "        [8, 8, 8, 8]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "test: torch.Tensor = torch.rand(3, 4)\n",
    "print(\"Random tensor (test)\")\n",
    "print(test)\n",
    "torch.zero_(test)\n",
    "print(\"Using the zero_() function on the random tensor\")\n",
    "print(test)\n",
    "print(\"Converting the previous tensor of zeros into one of ones\", torch.ones_like(test))\n",
    "\n",
    "zeroes: torch.Tensor = torch.zeros(2, 2)\n",
    "print(\"New tensor of zeroes using zeros()\")\n",
    "print(zeroes)\n",
    "\n",
    "ones: torch.Tensor = torch.ones(2, 2)\n",
    "print(\"New tensor of ones using ones()\")\n",
    "print(ones)\n",
    "\n",
    "in_range: torch.Tensor = torch.arange(start=1, end=9, step=2) #* Odd numbers from 1 to 8\n",
    "print(in_range)\n",
    "print(in_range.shape, in_range.ndim)\n",
    "\n",
    "empty_tensor: torch.Tensor = torch.empty_like(test)\n",
    "print(\"Supposedly an uninitialized tensor with the same shape as test\")\n",
    "print(empty_tensor)\n",
    "\n",
    "full_of_eights: torch.Tensor = torch.full_like(input=test, fill_value=8, dtype=torch.int16)\n",
    "print(\"A tensor with the same shape as test and with all of its entries = 8\")\n",
    "print(full_of_eights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.rand(Sequence[int])` is a way to create a tensor with random values from a uniform distribution in the range [0, 1) and shape `Sequence[int]`.\n",
    "- `torch.zeros(Sequence[int])` is a way to create a tensor filled with zeros and shape `Sequence[int]`. This will be useful in the future when creating masks, which filter out certain values in a tensor. A similar function is `torch.ones(Sequence[int])`, which creates a tensor filled with ones (not that used but may fulfill a similar purpose to the `zeros()` function).\n",
    "  - `torch.zero_(Tensor)` modifies the input tensor in-place to fill it with zeros (it also returns it).\n",
    "  - `torch.ones_like(Tensor)` creates a tensor filled with ones with the same shape as the input tensor.\n",
    "- `torch.arange(int, int, int)` creates a vector with values from `start` to `end` (exclusive) with a step of `step` (all integers).\n",
    "- `torch.empty_like(Tensor)` creates a tensor with the same shape as the input tensor but with uninitialized values (may look weird if printed).\n",
    "- `torch.full_like(Tensor, float)` creates a tensor with the same shape as the input tensor but filled with the value `float`.\n",
    "  - `torch.full(Sequence[int], float)` creates a tensor with the specified shape and filled with the value `float` without the need for another tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor data types (dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [1., 1.]], device='cuda:0', dtype=torch.float16, requires_grad=True)\n",
      "tensor([[2., 2.],\n",
      "        [1., 1.]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "cuda:0 torch.Size([2, 2]) torch.float64 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.device"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test: torch.Tensor = torch.tensor([[2, 2], [1, 1]],\n",
    "                                dtype=torch.half,\n",
    "                                device=\"cuda\",\n",
    "                                requires_grad=True)\n",
    "print(test)\n",
    "test_2: torch.Tensor = torch.tensor([[1, 1], [1, 1]],\n",
    "                                    dtype=torch.float64,\n",
    "                                    device=\"cuda\",\n",
    "                                    requires_grad=False)\n",
    "result: torch.Tensor = test * test_2\n",
    "print(result)\n",
    "print(result.device, result.shape, result.dtype, result.ndim)\n",
    "torch.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can get the device a tensor is stored in by using the `device` attribute. More importantly, you can set the default device for all tensors by using `torch.set_default_device(str)`. To temporary use it in a specific device, you can put your code inside a `with torch.device(str):` block (neither will affect the functions in which the device is specified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential errors when operating tensors\n",
    "\n",
    "1. `RuntimeError` associated with a difference in the size of the tensors in at least one dimension (**shape**). This is because the tensors must have the same size in each dimension to perform element-wise operations.\n",
    "2. `RuntimeError` expecting two or more tensors to be on the same device (*cuda* or *cpu*) but found at least a pair in different ones. This is because the tensors must be on the same device to perform element-wise operations.\n",
    "3. Probably `RuntimeError` as well regarding different data types of tensors being operated. Sometimes it happens and sometimes it doesn't. Needs to be confirmed in the future.\n",
    "   \n",
    "Note: When operating two tensors with different data types, the resulting tensor will have the data type of the tensor with the highest precision. In the same way, if a tensor requires a gradient, the resulting tensor will also require it even if the other one(s) don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar operations with tensors\n",
    "\n",
    "Think of these as the usual operations with a tensor and an element from a field but modified so that they are well-defined. For example, you can add a scalar to a tensor (or a tensor to a scalar) and it works by creating a tensor full of the scalars and of the same size or shape as the other tensor (could be done using `torch.full_like(Input_tensor, scalar)`. The same goes for the other operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6692, -0.1866],\n",
      "        [ 0.4688,  0.2123]])\n",
      "tensor([[4.6692, 3.8134],\n",
      "        [4.4688, 4.2123]])\n",
      "tensor([[-2.3308, -3.1866],\n",
      "        [-2.5312, -2.7877]])\n",
      "tensor([[ 0.3346, -0.0933],\n",
      "        [ 0.2344,  0.1061]])\n"
     ]
    }
   ],
   "source": [
    "tensor: torch.Tensor = torch.randn(size=[2, 2])\n",
    "print(tensor)\n",
    "addition_tensor: torch.Tensor = torch.add(tensor, 4) #* Same as torch.add(tensor, 4)\n",
    "print(addition_tensor)\n",
    "subtraction_tensor: torch.Tensor = tensor - 3\n",
    "print(subtraction_tensor)\n",
    "multip_tensor: torch.Tensor = (1/2) * tensor\n",
    "print(multip_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the operations element-wise together with tensors of the same shape are a field I believe, so the usual properties for real numbers with the usual operations should hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: `randn()` generates the numbers from a normal distribution with mean 0 and variance 1, whilst `rand()` generates them from a uniform distribution in the range [0, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication\n",
    "\n",
    "Soooo, as you may already know, matrix multiplication is the basis of neural networks, and doing it efficiently is what makes the technology viable. PyTorch devs are aware of this so they implemented the `torch.matmul()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  5,  5],\n",
      "        [10, 10, 10],\n",
      "        [15, 15, 15]])\n",
      "tensor([[ 5,  5,  5],\n",
      "        [10, 10, 10],\n",
      "        [15, 15, 15]])\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "tensor_1: torch.Tensor = torch.tensor([[1, 1], [2, 2], [3, 3]])\n",
    "tensor_2: torch.Tensor = torch.tensor([[2, 2, 2], [3, 3, 3]])\n",
    "result_with_matmul: torch.Tensor = torch.matmul(tensor_1, tensor_2)\n",
    "result_with_infix: torch.Tensor = tensor_1 @ tensor_2\n",
    "print(result_with_infix)\n",
    "print(result_with_matmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.matmul(Tensor, Tensor)` is used to multiply two tensors under some constraints, most notably in 2D, that the matrices are of size `n x m` and `m x p` respectively. It also has broadcasting, which allows you to multiply tensors of different shapes as long as they are compatible (which doesn't happen with `torch.mm()`)\n",
    "- Although the time it takes the interpreter to compute varies, it is more efficient in terms of big-O complexity than the usual for-loop implementation.\n",
    "- Seems like `torch.__matmul__(T, T)` is equivalent to `T @ T` (the matrix multiplication operator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose\n",
    "\n",
    "One way to avoid the mistake of computing the multiplication of tensors with different shapes is using their transpose. Now, this is generally a concept that makes sense in 2D, and it's just flipping a matrix. It can be done with the `torch.t()` function or the `Tensor.t()` method or its equivalent alias `Tensor.T`.\n",
    "\n",
    "In addition, it can be done on a higher-dimensional or one-dimensional tensor. To do so, you can use the `Tensor.permute([int])` method to indicate how do you want to reorder the dimensions. Currently (2024), `Tensor.T` does so reverting the shape, but it will be deprecated soon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0058],\n",
      "         [0.5430]],\n",
      "\n",
      "        [[0.6794],\n",
      "         [0.1083]],\n",
      "\n",
      "        [[0.7741],\n",
      "         [0.0140]]])\n",
      "tensor([[[0.0058],\n",
      "         [0.6794],\n",
      "         [0.7741]],\n",
      "\n",
      "        [[0.5430],\n",
      "         [0.1083],\n",
      "         [0.0140]]])\n",
      "tensor([[ 9, 21, 33],\n",
      "        [ 6, 14, 22],\n",
      "        [ 3,  7, 11]]) torch.Size([3, 3])\n",
      "tensor([[14, 20],\n",
      "        [14, 20]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor: torch.Tensor = torch.rand(3, 2, 1)\n",
    "print(tensor)\n",
    "print(tensor.permute(1, 0, 2)) #* Dimensions higher to smaller reading them left to righ ([2]=3, [1]=2, [0]=1)\n",
    "tensor_A: torch.Tensor = torch.tensor([[3, 3],\n",
    "                                       [2, 2],\n",
    "                                       [1, 1]])\n",
    "tensor_B: torch.Tensor = torch.tensor([[1, 2],\n",
    "                                       [3, 4],\n",
    "                                       [5, 6]])\n",
    "mul_1 = tensor_A @ tensor_B.T #* O: 3x3 matrix\n",
    "mul_2 = tensor_A.T @ tensor_B #* O: 2x2 matrix\n",
    "print(mul_1, mul_1.shape)\n",
    "print(mul_2, mul_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3680, 0.6320],\n",
      "        [0.3950, 0.6050]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def matrix_power(matrix, n):\n",
    "    result = matrix \n",
    "    for _ in range(n-1):\n",
    "        result = torch.matmul(matrix, result)\n",
    "    result = torch.round(result * 1000) / 1000\n",
    "    return result\n",
    "p: float = 0.40\n",
    "q: float = 1 - p\n",
    "matrix = torch.tensor([[0.2, 0.8],\n",
    "                       [0.5, 0.5]], dtype=torch.double)\n",
    "# print(matrix)\n",
    "matrix2: torch.Tensor = torch.matmul(matrix, matrix)\n",
    "vector: torch.Tensor = torch.tensor([0, 0, 1, 0, 0], dtype=torch.double)\n",
    "# print(torch.matmul(vector, matrix))\n",
    "print(matrix_power(matrix, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor aggregation\n",
    "\n",
    "Whenever you convert a tensor to a single scalar representing something of the values the tensor stores. Some of the most common operations that you can do are: finding the maximum/minimum value and their position, the mean or expected value and the sum of all the values in the tensor, etc.\n",
    "\n",
    ">Note: The tensor needs to be a float or complex dtype to compute its mean. Otherwise, an error message will pop up.\n",
    "\n",
    ">Note: The positional minimum and maximum will return a single scalar value representing the position from left to right and from top to bottom (therefore flattening the tensor) in the string represantation of the tensor.\n",
    "> If you want to know the exact position of the minimum and maximum, you may use the `dim` parameter to reduce the dimmensions to look the maximum/minimum for, and also set `keepdim` to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[75, 34, 74, 80, 63],\n",
      "        [ 5, 75, 68, 18, 25],\n",
      "        [13, 93, 50, 15, 25]]) torch.LongTensor\n",
      "Minimum at position 5 is 5\n",
      "Maximum at 11 is 93\n",
      "tensor([[1],\n",
      "        [0],\n",
      "        [0]])\n",
      "Mean: 47.53333282470703, sum: 713\n"
     ]
    }
   ],
   "source": [
    "tensor: torch.Tensor = torch.randint(low=1, high=99, size=[3, 5])\n",
    "print(tensor, tensor.type())\n",
    "#* Max and min\n",
    "minimum: int = tensor.min()\n",
    "maximum: int = torch.max(tensor) #*When doing aggregation, you seem to be able to call the operation from the module and input the tensor\n",
    "#* Positional max and min\n",
    "print(f\"Minimum at position {tensor.argmin()} is {minimum}\")\n",
    "print(f\"Maximum at {torch.argmax(tensor)} is {maximum}\")\n",
    "print(tensor.argmin(dim=1, keepdim=True))\n",
    "#* Mean and sum\n",
    "mean: float = tensor.type(torch.float32).mean() #!Must be complex or float, so it needs to be casted in this case\n",
    "sum: int = torch.sum(tensor)\n",
    "print(f\"Mean: {mean}, sum: {sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping, viewing, (un)squeezing, and permuting tensors\n",
    "\n",
    "- Reshape: Modifies current tensor to a new shape that's compatible with the previous one, i.e., one in which you can fit the same elements you had in the initial tensor. It does so by traversing the tensor from left to right and top to bottom to create a new tensor with a shape `[n1, n2, ..., nj]`, where `n1 * n2 * ... * nj = size1 * size2 * ... * sizen` (the new size is compatible with the current size).\n",
    "  \n",
    "  It also returns a copy of the reshaped tensor or a view if it's compatible with the last shape.\n",
    "- View: Creates a new tensor that holds a reference to the base one, so whenever you change either (particularly its data), the other one will change.\n",
    "- Stack: Stacks a sequence of tensors on a new dimension. The default dimension, `dim`, is 0, and the results you get when you change it are not as intuitive as you may think, so it really depends on particular use cases.\n",
    "  - Hstack: Stacks \"horizontally\" a sequence of tensors. It is equivalent to `.cat(dim = n)` (concatenate in dimension `n`) for `n = 0` for 1D tensors, `n = 1` for 2D tensors, and `n = -1` for higher-dimmension tensors. What you essentially do is you take each element of the last dimmension and create a horizontal \"vector\" with each component of the element if necessary where the bottom element is the right-most. If the element only has one column, then you leave it as it is. After you do the same with the other tensors left in the sequence, you put them side-by-side in the same order of the sequence and concatanate each \"row\" of all the vectors into one.\n",
    "  - Vstack: Stacks \"vertically\" a sequence of tensors. Essentially, you create the same \"vectors\" as in the hstack but you place them one below the other instead of side-by-side, and concatenate them in a single column instead of a single row.\n",
    "- Squeeze: Removes all the 1D dimensions (or only the specified ones) and \"joins\" the other ones in contiguous dimensions. Visually, what it does is it removes any pair or n-uple of consecutive squared brackets that have no information between them, so they are essentially unnecessary.\n",
    "- Unsqueeze: Transforms the tensor so that a new dimension is added at `dim`. However, `dim` cannot exceed the dimension of the tensor plus 1.\n",
    "- Permute: Returns a view of the tensor with the dimensions permuted in the specified order but maintaining the same information. This means if you want to access the element `x[0, 1, 2]` and you permute the dimensions as `(1, 0, 2)` (so dimension 1 goes to dimension 0, 0 goes to 1, and 2 remains the same), the same element can now be accessed as `x[1, 0, 2]`.\n",
    "\n",
    "> `Tensor.contiguous()` returns the same tensor if it's already contiguous or a deep copy of it that's contiguous.\n",
    "> Being contiguous means it is stored in contiguous memory positions, just like an array. This may be an advantage when doing computations between tensors. You can check if the tensor is contiguous using `Tensor.is_contiguous()`, which is relevant since some operations on tensors return non-contiguous ones, like `.T` (transpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[     19,       6,       5],\n",
      "         [     23,      19, 8923842]],\n",
      "\n",
      "        [[     12,      23,      19],\n",
      "         [     10,       6,      20]],\n",
      "\n",
      "        [[     23,      13,       2],\n",
      "         [      4,       3,      22]]])\n",
      "tensor([[[     19,      12,      23],\n",
      "         [      6,      23,      13],\n",
      "         [      5,      19,       2]],\n",
      "\n",
      "        [[     23,      10,       4],\n",
      "         [     19,       6,       3],\n",
      "         [8923842,      20,      22]]])\n"
     ]
    }
   ],
   "source": [
    "# print(tensor)\n",
    "# print(\"Reshape\", tensor.reshape([5, 3]))\n",
    "tensor_1 = torch.randint(low=1, high=30, size=[3, 2, 3])\n",
    "tensor_1[0, 1, 2] = 8923842\n",
    "tensor_2 = torch.randint(low=1, high=30, size=[3, 2, 3])\n",
    "print(tensor_1)\n",
    "# print(tensor_2)\n",
    "# print(\"Stack:\", torch.stack([tensor_1, tensor_2], dim=1))\n",
    "# print(\"Horizontal stack:\", torch.hstack([tensor_1, tensor_2]))\n",
    "# print(\"Vertical stack:\", torch.vstack([tensor_1, tensor_2]))\n",
    "zeroes = torch.zeros([3, 1, 3])\n",
    "# print(zeroes)\n",
    "squeezed: torch.Tensor = zeroes.squeeze()\n",
    "# print(squeezed.unsqueeze(dim=2))\n",
    "permuted = tensor_1.permute(1, 2, 0)\n",
    "print(permuted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "There are two ways to do it (the names are unofficial but do match the context):\n",
    "\n",
    "### Chained\n",
    "\n",
    "You access the i-th position of the j-th dimension by placing `i` inside square brackets after doing the same with the other `j-1` dimension: `tensor[a][b]...[j-1][i]`\n",
    "\n",
    "This syntax is widely used when working with arrays of arrays or numpy arrays.\n",
    "\n",
    "For example, if you want to access the 1st element of the 1st dimension of the 0th dimension in a 3D tensor, you can do so with the syntax `tensor[0][1][1]`.\n",
    "\n",
    "### Tuple\n",
    "\n",
    "You access the i-th position of the j-th dimension by placing all the values of the `j-1`-th dimensions inside a single pair of square brackets, separated by commas: `tensor[a, b, ..., j-1, i]`\n",
    "\n",
    "For instance, the same element you wanted to access in the chained indexing example you can use the syntax `tensor[0, 1, 1]`.\n",
    "\n",
    "This one is, on average, **marginally faster** because it doesn't need to break the tensor into sub-tensors as chained indexing does.\n",
    "\n",
    "\n",
    "### The colon (`:`)\n",
    "\n",
    "It's used when you want to sort of 'carry' an entire dimension and access all the elements of it and not just one (see example below). **It can only be used with tuple indexing**.\n",
    "\n",
    "> When used at the last index, it is equivalent to removing it (it's, to make it easier to understand, implicitly called)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]]])\n",
      "tensor(1) tensor(1)\n",
      "tensor([1, 2, 3, 4]) tensor([1, 5, 9])\n",
      "tensor([1, 2, 3, 4]) tensor([1, 2, 3, 4])\n",
      "tensor([[ 4,  8, 12]])\n",
      "tensor([12])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(1, 13).reshape(1, 3, 4)\n",
    "print(tensor)\n",
    "\n",
    "#*Accessing the top-left element\n",
    "print(tensor[0][0][0], tensor[0, 0, 0])\n",
    "\n",
    "#*The colo syntax only works for tuple indexing\n",
    "print(tensor[0][:][0], tensor[0, :, 0]) #!Different\n",
    "\n",
    "#*The colon is redundant when used at the end\n",
    "print(tensor[0, 0], tensor[0, 0, :])\n",
    "\n",
    "#*Accessing the last \"column\" of elements\n",
    "print(tensor[:, :, 3])\n",
    "\n",
    "#*When the first dimension has only one element (a matrix in this case), using the colon just adds squared brackets to the element you're trying to access\n",
    "print(tensor[:, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From and to NumPy\n",
    "\n",
    "NumPy is the best and most-widely used library to do computations with any and lots of data. Therefore, it makes a lot of sense that PyTorch and NumPy join forces.\n",
    "\n",
    "This is mainly done by switching the data from one library to another using two PyTorch methods:\n",
    "\n",
    "#### `torch.from_numpy(ndarray)`\n",
    "\n",
    "This method/function returns a Tensor object containing the same data and shape as the ndarray input. However, the two don't share memory, so the Tensor is a *deep-copy* of the ndarray.\n",
    "\n",
    "> Something to bear in mind is that, by default, the Tensor will have a `float64` data type (unless converted), which is the default data type in numpy.\n",
    "\n",
    "#### `torch.Tensor.numpy()`\n",
    "\n",
    "This function returns an ndarray containing the same data and shape as the tensor the method is called from. In the same way as the other method, the two don't share memory.\n",
    "\n",
    "It has some requirements regarding the data type (compatible with NumPy), the tensor grad and bits, and the location of the tensor (GPU or CPU). If some of those requirements are not met, you may pass `True` as an argument to set `force` to true, which will execute a series of methods that may make the tensor compatible with an ndarray.\n",
    "\n",
    "> Just like from NumPy to Torch, the default dtype of the ndarray will be `float32`, since this is a tensor's default dtype.\n",
    ">\n",
    "> IMPORTANT: The tensor must be stored in the CPU. Otherwise, the conversion to ndarray cannot be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]] float64\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) torch.float32\n",
      "---\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]]]) torch.float32\n",
      "[[[1. 2. 3.]\n",
      "  [4. 5. 6.]\n",
      "  [7. 8. 9.]]] float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "array_of_ones: np.ndarray = np.ones((2, 3))\n",
    "print(array_of_ones, array_of_ones.dtype)\n",
    "ones_tensor: torch.Tensor = torch.from_numpy(array_of_ones).type(torch.float32) #*You can use .type(torch.some_dtype) to convert the output tensor to the type you need (default is float32)\n",
    "print(ones_tensor, ones_tensor.dtype)\n",
    "\n",
    "print(\"---\")\n",
    "tensor = torch.arange(1., 10.).reshape(1, 3, 3)\n",
    "array_from_tensor: np.ndarray = tensor.numpy()\n",
    "print(tensor, tensor.dtype)\n",
    "print(array_from_tensor, array_from_tensor.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    "It refers to a random tensor's ability to be reproduced at any time and machine, i.e., to be duplicated by anyone using any computer while still being pseudo-random.\n",
    "\n",
    "This is done setting a random seed, which is a vector or number used to initialize a pseudo-random number generator (PRNG). In turn, the same generator should always output the same sequence of numbers allocated for a particular seed. This means, when not setting a seed, the seed is randomized every time a random number is needed, which is why you get different numbers almost every time you use `torch.rand` or any other PRNG.\n",
    "\n",
    "In PyTorch (and probably in general), the seed must be an integer. If a floating point is passed, then it is rounded to the closest integer.\n",
    "\n",
    "> In PyTorch, you use the `torch.manual_seed(seed)` to make the next random tensor reproducible. You only have to call it at the start of a file, but in Jupyter notebooks, it needs to be called before a random method every time you use one.\n",
    ">\n",
    "> If you want to set a seed only for the current GPU, you can use `torch.cuda.manual_seed(seed)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009]])\n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009]])\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED: int = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor1: torch.Tensor = torch.rand(2, 3)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor2: torch.Tensor = torch.rand(2, 3)\n",
    "print(random_tensor1)\n",
    "print(random_tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device-agnostic code\n",
    "\n",
    "As you may guess by its name, device-agnostic code allows you to run code no matter the device you are working with (CPU or GPU). It is a way to condition the way your code runs without affecting the code itself, but only where tensors are stored.\n",
    "\n",
    "There are two type of devices: `cpu` and `cuda`, the latter standing for GPUs or TPUs compatible with CUDA.\n",
    "\n",
    "You can initialize a tensor with the device parameter and it will store the tensor in the argument passed. Also, if you want to make all the computations of a file in one of the devices (whether it is the default or not it is always to explicitly state it, since in the future you may want to change it), you can use the `torch.device(device)` method and pass the desired device as the argument.\n",
    "\n",
    "The most comprehensible and shortest standard to make device-agnostic code (there is another one in the docs but it involves more step and is visually more confusing), is by setting a `device` variable at the start of the code to a conditional depending on whether a GPU is available or not. In consequence, the code will run on a GPU every time it's available, and on a CPU otherwise.\n",
    "\n",
    "> If a CUDA-compatible device is available, then it will print `cuda:0`, where `0` may be another integer representing the index of the device (there could be multiple GPUs or TPUs being used)\n",
    "\n",
    "#### Switching between the two\n",
    "\n",
    "If you want a **deep copy** of a tensor stored in the other device the tensor is currently in, you can use:\n",
    "- `Tensor.cpu()` to go from CUDA to CPU.\n",
    "- `Tensor.cuda()` to go from CPU to CUDA.\n",
    "\n",
    "> Bear in mind both methods will return `self` if the tensor is already in the device you are converting to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda\n",
      "tensor([[0.6130, 0.0101],\n",
      "        [0.3984, 0.0403]], device='cuda:0')\n",
      "tensor([[0.9877, 0.1289],\n",
      "        [0.5621, 0.5221]], device='cuda:0')\n",
      "[[0.61295986 0.01005884]\n",
      " [0.3984137  0.0403084 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Available device:\", device)\n",
    "\n",
    "#*Setting a device for a particular tensor\n",
    "tensor = torch.rand(size=[2, 2], device=device)\n",
    "print(tensor)\n",
    "\n",
    "#*Setting a device for all tensors from here to the end of the file\n",
    "torch.set_default_device(device)\n",
    "tensor_1 = torch.rand(size=[2, 2])\n",
    "print(tensor_1)\n",
    "\n",
    "#!To convert to numpy arrays, the tensor must be stored in the CPU\n",
    "ndarray: np.ndarray = tensor.cpu().numpy()\n",
    "print(ndarray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
